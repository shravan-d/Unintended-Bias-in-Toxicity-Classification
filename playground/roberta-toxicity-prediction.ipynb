{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12500,"databundleVersionId":1375107,"sourceType":"competition"}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Initial Toxicity predictions with RoBERTa\n### Running times are a main concern for later expanding how much data we use, but for now will use this as basis for building some bias detection ","metadata":{}},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T23:44:28.066486Z","iopub.execute_input":"2024-12-07T23:44:28.066997Z","iopub.status.idle":"2024-12-07T23:44:28.076065Z","shell.execute_reply.started":"2024-12-07T23:44:28.066968Z","shell.execute_reply":"2024-12-07T23:44:28.075292Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv\n/kaggle/input/jigsaw-unintended-bias-in-toxicity-classification/all_data.csv\n/kaggle/input/jigsaw-unintended-bias-in-toxicity-classification/test_public_expanded.csv\n/kaggle/input/jigsaw-unintended-bias-in-toxicity-classification/test_private_expanded.csv\n/kaggle/input/jigsaw-unintended-bias-in-toxicity-classification/toxicity_individual_annotations.csv\n/kaggle/input/jigsaw-unintended-bias-in-toxicity-classification/train.csv\n/kaggle/input/jigsaw-unintended-bias-in-toxicity-classification/identity_individual_annotations.csv\n/kaggle/input/jigsaw-unintended-bias-in-toxicity-classification/test.csv\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T07:33:35.097149Z","iopub.execute_input":"2024-12-08T07:33:35.097441Z","iopub.status.idle":"2024-12-08T07:33:35.101230Z","shell.execute_reply.started":"2024-12-08T07:33:35.097415Z","shell.execute_reply":"2024-12-08T07:33:35.100331Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"filename = '/kaggle/input/jigsaw-unintended-bias-in-toxicity-classification/all_data.csv'\n\ndf = pd.read_csv(filename)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T07:37:57.412053Z","iopub.execute_input":"2024-12-08T07:37:57.412376Z","iopub.status.idle":"2024-12-08T07:38:24.094877Z","shell.execute_reply.started":"2024-12-08T07:37:57.412344Z","shell.execute_reply":"2024-12-08T07:38:24.094148Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import sklearn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score\n# from sklearn.feature_extraction.text import TfidfVectorizer\nimport re\nfrom tqdm import tqdm\nfrom transformers import RobertaTokenizer, RobertaModel, RobertaForSequenceClassification\nimport torch\nfrom torch.utils.data import DataLoader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T07:38:37.760837Z","iopub.execute_input":"2024-12-08T07:38:37.761164Z","iopub.status.idle":"2024-12-08T07:38:54.424303Z","shell.execute_reply.started":"2024-12-08T07:38:37.761132Z","shell.execute_reply":"2024-12-08T07:38:54.423353Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nroberta = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=1)  # Regression task","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T07:39:01.490545Z","iopub.execute_input":"2024-12-08T07:39:01.491876Z","iopub.status.idle":"2024-12-08T07:39:05.772481Z","shell.execute_reply.started":"2024-12-08T07:39:01.491823Z","shell.execute_reply":"2024-12-08T07:39:05.771584Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11ab68d933834d3b8cd54c2badd9b02e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a21b553543a4c01a4b78b42474768ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdba0b50edb841f99f42f3cbe3a3b7be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"519bc48dfe504b5789215bb1a122bf95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20a1db9cfb5b4b4ca936ed065e4540f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa434d2b3ecf4bc89ccf81e835c6edee"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Preprocessing\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n    return text\n\ndf['comment_text'] = df['comment_text'].fillna('') # Remove NaN values for TDIDF\ndf['cleaned_comment'] = df['comment_text'].apply(clean_text) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T07:39:08.815449Z","iopub.execute_input":"2024-12-08T07:39:08.816187Z","iopub.status.idle":"2024-12-08T07:39:24.746858Z","shell.execute_reply.started":"2024-12-08T07:39:08.816155Z","shell.execute_reply":"2024-12-08T07:39:24.746126Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Separate train/test data\ntraindf = df[df['split'] == 'train']\ntestdf =  df[df['split'] == 'test']\n\nXtrain = traindf['cleaned_comment'] \nytrain = traindf['toxicity'] # target\nXtest = testdf['cleaned_comment']\nytest = testdf['toxicity']\n\n# Downsize dataset for reasonable runtimes\nXtrain = Xtrain[:1000]\nXtest = Xtest[:200]\nytrain = ytrain[:1000]\nytest = ytest[:200]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T07:45:02.749713Z","iopub.execute_input":"2024-12-08T07:45:02.750653Z","iopub.status.idle":"2024-12-08T07:45:03.598530Z","shell.execute_reply.started":"2024-12-08T07:45:02.750608Z","shell.execute_reply":"2024-12-08T07:45:03.597757Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Tokenize with Roberta\n\n# As opposed to tfidf approach, do not remove stopwords for better context using roberta\nXtrain_r = traindf['cleaned_comment'].tolist()\nytrain_r = traindf['toxicity'].tolist()\nXtest_r = testdf['cleaned_comment'].tolist()\nytest_r = testdf['toxicity'].tolist()\n\n# Downsize dataset for reasonable runtimes\nXtrain_r = Xtrain_r[:1000]\nXtest_r = Xtest_r[:200]\nytrain_r = ytrain_r[:1000]\nytest_r = ytest_r[:200]\n\nXtrain_encodings = tokenizer(Xtrain_r, truncation=True, padding=True, max_length=200, return_tensors='pt') # choosing length of comment\nXtest_encodings = tokenizer(Xtest_r, truncation=True, padding=True, max_length=200, return_tensors='pt')\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T07:45:11.567086Z","iopub.execute_input":"2024-12-08T07:45:11.567428Z","iopub.status.idle":"2024-12-08T07:45:12.301520Z","shell.execute_reply.started":"2024-12-08T07:45:11.567402Z","shell.execute_reply":"2024-12-08T07:45:12.300606Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Convert to tensors to prepare for dataloader \nytrain_tensor = torch.tensor(ytrain_r, dtype=torch.float)\nytest_tensor = torch.tensor(ytest_r, dtype=torch.float)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T07:45:18.515090Z","iopub.execute_input":"2024-12-08T07:45:18.515732Z","iopub.status.idle":"2024-12-08T07:45:18.519902Z","shell.execute_reply.started":"2024-12-08T07:45:18.515694Z","shell.execute_reply":"2024-12-08T07:45:18.519009Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"Xtraintorch = torch.utils.data.TensorDataset(Xtrain_encodings['input_ids'], Xtrain_encodings['attention_mask'], ytrain_tensor)\nXtesttorch = torch.utils.data.TensorDataset(Xtest_encodings['input_ids'], Xtest_encodings['attention_mask'], ytest_tensor)\n# Try 8 batch size to reduce running time\ntrain_dataloader = DataLoader(Xtraintorch, batch_size=8, shuffle=True)\ntest_dataloader = DataLoader(Xtesttorch, batch_size=8, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T07:45:22.509163Z","iopub.execute_input":"2024-12-08T07:45:22.509507Z","iopub.status.idle":"2024-12-08T07:45:22.514702Z","shell.execute_reply.started":"2024-12-08T07:45:22.509477Z","shell.execute_reply":"2024-12-08T07:45:22.513823Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"### With batch size 8, 1000 rows of training data, 1 epoch: 10 min running time to train w Roberta","metadata":{}},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(roberta.parameters(), lr=1e-5)\nroberta.train()\n\nfor epoch in range(1):  #Testing with 1 initially \n    for batch in tqdm(train_dataloader):\n        optimizer.zero_grad()  # Clear previous gradients\n        \n        # Assign input data and labels from batch\n        input_ids = batch[0]\n        attention_mask = batch[1]\n        labels = batch[2]\n        \n        # Forward pass: Compute predictions and loss\n        outputs = roberta(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()  # backpropagate loss\n        optimizer.step() \n        \n    print(f\"Epoch {epoch + 1}: Loss {loss.item()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T07:45:30.163606Z","iopub.execute_input":"2024-12-08T07:45:30.163976Z","iopub.status.idle":"2024-12-08T07:55:07.855253Z","shell.execute_reply.started":"2024-12-08T07:45:30.163944Z","shell.execute_reply":"2024-12-08T07:55:07.854305Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 125/125 [09:37<00:00,  4.62s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Loss 0.05774139612913132\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"def evaluate_model(model, test_dataloader):\n    ''' Evaluate model on test data using same framework as the training loop\n    Params: model: torch.nn.Module, test_dataloader: torch.dataloader \n    Output: tuple: pred (list) of predicted toxicity scores for the test data, actual (list) of true scores\n    '''\n    model.eval()\n    pred = []\n    actual = []\n    \n    with torch.no_grad(): # no gradient calculation for faster running \n        for batch in tqdm(test_dataloader):\n            # Get the input data and labels from the batch\n            input_ids = batch[0]\n            attention_mask = batch[1]\n            labels = batch[2]\n            \n            # Forward pass: Compute predictions\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n            logits = outputs.logits\n\n            pred.extend(logits.cpu().numpy())\n            actual.extend(labels.cpu().numpy()) \n\n    return pred, actual\n            \n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T08:29:34.899561Z","iopub.execute_input":"2024-12-08T08:29:34.900414Z","iopub.status.idle":"2024-12-08T08:29:34.907845Z","shell.execute_reply.started":"2024-12-08T08:29:34.900362Z","shell.execute_reply":"2024-12-08T08:29:34.906781Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"pred1, actual1 = evaluate_model(roberta, test_dataloader)\n\n# Calculate MSE of roberta model, trained with batch size 8, 200 rows of test data, 1 epoch\nmse1 = mean_squared_error(actual1, pred1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T08:31:06.561866Z","iopub.execute_input":"2024-12-08T08:31:06.562951Z","iopub.status.idle":"2024-12-08T08:31:38.391906Z","shell.execute_reply.started":"2024-12-08T08:31:06.562901Z","shell.execute_reply":"2024-12-08T08:31:38.390849Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 25/25 [00:31<00:00,  1.27s/it]\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"print(f'Mean Squared Error for Roberta model using 1000/20 data split: {mse1}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T08:32:27.967177Z","iopub.execute_input":"2024-12-08T08:32:27.967503Z","iopub.status.idle":"2024-12-08T08:32:27.972110Z","shell.execute_reply.started":"2024-12-08T08:32:27.967472Z","shell.execute_reply":"2024-12-08T08:32:27.971321Z"}},"outputs":[{"name":"stdout","text":"Mean Squared Error for Roberta model using 1000/20 data split: 0.06018800660967827\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"### Low MSE and Loss on Roberta toxicity predictions. Now addressing unintended bias in predictions:","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}