{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias mitigated Toxicity predictions with RoBERTa\n",
    "# Necessary files: roberta2_finetuned (folder), dataset.py, all_data.csv\n",
    "#### Running times are a main concern for later expanding how much data we use, but for now will use this as basis for building some bias detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T00:42:25.485243Z",
     "iopub.status.busy": "2024-12-10T00:42:25.484347Z",
     "iopub.status.idle": "2024-12-10T00:42:38.774145Z",
     "shell.execute_reply": "2024-12-10T00:42:38.773130Z",
     "shell.execute_reply.started": "2024-12-10T00:42:25.485210Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T00:42:38.776474Z",
     "iopub.status.busy": "2024-12-10T00:42:38.775914Z",
     "iopub.status.idle": "2024-12-10T00:43:02.454983Z",
     "shell.execute_reply": "2024-12-10T00:43:02.454111Z",
     "shell.execute_reply.started": "2024-12-10T00:42:38.776443Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('all_data.csv')\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "roberta = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=1)  # Regression task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T00:22:25.048778Z",
     "iopub.status.busy": "2024-12-10T00:22:25.048055Z",
     "iopub.status.idle": "2024-12-10T00:22:25.972780Z",
     "shell.execute_reply": "2024-12-10T00:22:25.972083Z",
     "shell.execute_reply.started": "2024-12-10T00:22:25.048734Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T00:27:08.297573Z",
     "iopub.status.busy": "2024-12-10T00:27:08.297199Z",
     "iopub.status.idle": "2024-12-10T00:27:23.917171Z",
     "shell.execute_reply": "2024-12-10T00:27:23.916452Z",
     "shell.execute_reply.started": "2024-12-10T00:27:08.297542Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df['comment_text'] = df['comment_text'].fillna('') # Remove NaN values\n",
    "df['cleaned_comment'] = df['comment_text'].apply(clean_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T00:27:24.787031Z",
     "iopub.status.busy": "2024-12-10T00:27:24.786700Z",
     "iopub.status.idle": "2024-12-10T00:27:27.805384Z",
     "shell.execute_reply": "2024-12-10T00:27:27.804360Z",
     "shell.execute_reply.started": "2024-12-10T00:27:24.787001Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29992 159782\n"
     ]
    }
   ],
   "source": [
    "df['target'] = df['toxicity'] # Mark target col\n",
    "data = dataset.split_dataframe(df)\n",
    "\n",
    "# Assign dfs\n",
    "traindf = data[0]\n",
    "testdf = data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using smaller subset \n",
    "trainsubset = traindf[:25000]\n",
    "testsubset = testdf[:2500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1060731/3282448567.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  trainsubset[identities] = trainsubset[identities].fillna(0.0)\n",
      "/tmp/ipykernel_1060731/3282448567.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  testsubset[identities] = testsubset[identities].fillna(0.0)\n"
     ]
    }
   ],
   "source": [
    "# Treat NaN values in identity cols\n",
    "identities = ['male', 'female', 'transgender',\n",
    "       'other_gender', 'heterosexual', 'homosexual_gay_or_lesbian', 'bisexual',\n",
    "       'other_sexual_orientation', 'christian', 'jewish', 'muslim', 'hindu',\n",
    "       'buddhist', 'atheist', 'other_religion', 'black', 'white', 'asian',\n",
    "       'latino', 'other_race_or_ethnicity', 'physical_disability',\n",
    "       'intellectual_or_learning_disability', 'psychiatric_or_mental_illness',\n",
    "       'other_disability']\n",
    "\n",
    "trainsubset[identities] = trainsubset[identities].fillna(0.0)\n",
    "\n",
    "testsubset[identities] = testsubset[identities].fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def adversarialloss(preds, identities_col_agg):\n",
    "    return f.mse_loss(preds.flatten(), identities_col_agg.flatten())\n",
    "\n",
    "def biasloss(modelchoice, input_ids, attention_mask, labels, identities_col, bias_penalty_factor=0.5):\n",
    "    '''Calculates loss based on model's toxicity prediction with the bias penalty \n",
    "     bias_penalty_factor: factor for how much to penalize bias '''\n",
    "    logits = modelchoice(input_ids, attention_mask=attention_mask).logits\n",
    "    loss = f.mse_loss(logits.flatten(), labels)\n",
    "    # Adversarial loss\n",
    "    adversary_logits = modelchoice(input_ids, attention_mask=attention_mask).logits  \n",
    "    adversary_loss = adversarialloss(adversary_logits, identities_col)\n",
    "    \n",
    "    # Calculate with adversarial penalty\n",
    "    total_loss = loss - bias_penalty_factor * adversary_loss\n",
    "    \n",
    "    return total_loss\n",
    "def train_with_penalty(model, train_dataloader, optimizer, device, bias_penalty_factor=0.5):\n",
    "    ''' Training roberta model with adversarial loss to address bias'''\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        for batch in tqdm(train_dataloader):\n",
    "            optimizer.zero_grad()  # Clear gradients\n",
    "\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    "            identities_col = batch[3].to(device)  \n",
    "                        \n",
    "            # Compute the loss with bias regularization\n",
    "            loss = biasloss(model, input_ids, attention_mask, labels, identities_col, bias_penalty_factor)\n",
    "            loss.backward() \n",
    "            optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}: Loss {loss.item()}\")\n",
    "\n",
    "Xtrain_r2 = list(trainsubset['cleaned_comment'])\n",
    "ytrain_r2 = list(trainsubset['toxicity'])\n",
    "Xtest_r2 = list(testsubset['cleaned_comment'])\n",
    "ytest_r2 = list(testsubset['toxicity'])\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "roberta2 = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=1)  # Regression task\n",
    "\n",
    "Xtrain_encodings = tokenizer(Xtrain_r2, truncation=True, padding=True, max_length=200, return_tensors='pt') # choosing max length of comment\n",
    "Xtest_encodings = tokenizer(Xtest_r2, truncation=True, padding=True, max_length=200, return_tensors='pt')\n",
    "\n",
    "# Convert to tensors to prepare for dataloader \n",
    "ytrain_tensor = torch.tensor(ytrain_r2, dtype=torch.float)\n",
    "ytest_tensor = torch.tensor(ytest_r2, dtype=torch.float)\n",
    "\n",
    "identities_col_train = torch.tensor(trainsubset[identities].values, dtype=torch.float)\n",
    "identities_col_test = torch.tensor(testsubset[identities].values, dtype=torch.float)\n",
    "\n",
    "# Penalizing the loss on any identity label presence or absence of minority and adjust the loss accordingly\n",
    "identities_col_train_agg = (identities_col_train.any(dim=1).float()).unsqueeze(1)\n",
    "identities_col_test_agg  = (identities_col_test.any(dim=1).float()).unsqueeze(1)\n",
    "\n",
    "\n",
    "Xtraintorch = TensorDataset(Xtrain_encodings['input_ids'], Xtrain_encodings['attention_mask'], ytrain_tensor, identities_col_train_agg)\n",
    "Xtesttorch = TensorDataset(Xtest_encodings['input_ids'], Xtest_encodings['attention_mask'], ytest_tensor, identities_col_test_agg)\n",
    "# Try different batch size to reduce running time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply bias mitigation with adversarial loss to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    roberta2 = nn.DataParallel(roberta2)\n",
    "\n",
    "roberta2 = roberta2.to(device)\n",
    "optimizer = torch.optim.AdamW(roberta2.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(Xtraintorch, batch_size=164, shuffle=True)\n",
    "test_dataloader = DataLoader(Xtesttorch, batch_size=164, shuffle=False)\n",
    "# train_with_penalty(roberta2, train_dataloader, optimizer, device, bias_penalty_factor=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "#  roberta2.module.save_pretrained('roberta2_finetuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:05<00:00,  2.93it/s]\n"
     ]
    }
   ],
   "source": [
    "## For inference\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "inference_model = RobertaForSequenceClassification.from_pretrained('roberta2_finetuned').to(device)\n",
    "\n",
    "def evaluate_model(model, test_dataloader):\n",
    "    ''' Evaluate model on test data using same framework as the training loop\n",
    "    Params: model: torch.nn.Module, test_dataloader: torch.dataloader \n",
    "    Output: tuple: pred (list) of predicted toxicity scores for the test data, actual (list) of true scores\n",
    "    '''\n",
    "    model.eval()\n",
    "    pred = []\n",
    "    actual = []\n",
    "    \n",
    "    with torch.no_grad(): # no gradient calculation for faster running \n",
    "        for batch in tqdm(test_dataloader):\n",
    "            # Get the input data and labels from the batch\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    "            \n",
    "            # Forward pass: Compute predictions\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            pred.extend(logits.cpu().numpy())\n",
    "            actual.extend(labels.cpu().numpy()) \n",
    "\n",
    "    return pred, actual\n",
    "\n",
    "pred2, actual2 = evaluate_model(inference_model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error for Roberta bias mitigated model using 25000/2500 data split: 0.37158588833321393\n"
     ]
    }
   ],
   "source": [
    "mse2 = mean_squared_error(actual2, pred2)\n",
    "print(f'Mean Squared Error for Roberta bias mitigated model using 25000/2500 data split: {mse2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Bias-mitigated Binary Classification with threshold of 0.4 : 100.00%\n"
     ]
    }
   ],
   "source": [
    "binary_preds2 = [1 if pred>=0.4 in pred2 else 0 for pred in pred2]\n",
    "binary_labels2 = [1 if label>=0.4 in actual2 else 0 for label in actual2]\n",
    "\n",
    "print(f'Accuracy for Bias-mitigated Binary Classification with threshold of '\n",
    "        f'0.4 : {100 * accuracy_score(binary_labels2, binary_preds2):0.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 1375107,
     "sourceId": 12500,
     "sourceType": "competition"
    },
    {
     "datasetId": 6267404,
     "sourceId": 10151877,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "nadia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
