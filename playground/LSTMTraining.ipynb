{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12500,"databundleVersionId":1375107,"sourceType":"competition"},{"sourceId":9855271,"sourceType":"datasetVersion","datasetId":6047831}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install contractions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T22:33:46.159152Z","iopub.execute_input":"2024-12-08T22:33:46.159517Z","iopub.status.idle":"2024-12-08T22:33:56.128282Z","shell.execute_reply.started":"2024-12-08T22:33:46.159476Z","shell.execute_reply":"2024-12-08T22:33:56.127456Z"}},"outputs":[{"name":"stdout","text":"Collecting contractions\n  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\nCollecting textsearch>=0.0.21 (from contractions)\n  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\nCollecting anyascii (from textsearch>=0.0.21->contractions)\n  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\nCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n  Downloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (13 kB)\nDownloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\nDownloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\nDownloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\nSuccessfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom sklearn.metrics import accuracy_score\nimport torch.optim as optim\nimport pandas as pd\nimport re\nfrom torch.utils.data import Dataset, DataLoader\nfrom nltk.tokenize import word_tokenize, WhitespaceTokenizer\nfrom contractions import fix\nimport numpy as np\nfrom collections import Counter\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom torch.nn import functional as F","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T22:33:56.130192Z","iopub.execute_input":"2024-12-08T22:33:56.130471Z","iopub.status.idle":"2024-12-08T22:34:00.435583Z","shell.execute_reply.started":"2024-12-08T22:33:56.130444Z","shell.execute_reply":"2024-12-08T22:34:00.434747Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"class JigsawDataset(Dataset):\n    def __init__(self, comments, labels, glove_vocab, max_length):\n        self.texts = comments.tolist()\n        self.labeAls = labels.tolist()\n        self.glove_vocab = glove_vocab\n        self.max_length = max_length\n        self.tokenizer = WhitespaceTokenizer()\n        self.processed_texts = [self._preprocess(text) for text in self.texts]\n\n    def _preprocess(self, text):\n        # Expand contractions\n        text = fix(text)\n        # Convert to lower case\n        text = text.lower()\n        # Replace underscores with spaces\n        text = re.sub(r'[_]', ' ', text)\n        # Removing characters that usually don't add meaning to a sentence\n        text = re.sub(r\"[^?$.-:()%@!&=+/><,a-zA-Z\\s0-9\\w]\", '', text)\n        # Changes multiple occurrences of these special characters to only one occurrence. For example '???' to '?'\n        text = re.sub(r'([?.!#$%&()*+,-/:;_<=>@[^`|])\\1+', r'\\1', text)\n        # Inserts a space before and after special characters so embeddings can catch them\n        text = re.sub(r'([?.!#$%&()*+,-/:;_<=>@[^`|])', r' \\1 ', text)\n        # Removes extra spaces that may have come in from the previous operation\n        text = re.sub(r'([\\s])\\1+', r'\\1', text)\n        # Tokenize\n        tokens = self.tokenizer.tokenize(text)\n        # Filter tokens not in GloVe vocab\n        tokens = [token if token in self.glove_vocab else '<unk>' for token in tokens]\n        return tokens\n\n    def __len__(self):\n        return len(self.processed_texts)\n\n    def __getitem__(self, idx):\n        # TODO: Update this function based on the format required for the model\n        tokens = self.processed_texts[idx]\n        # Pad or truncate to max_length\n        if len(tokens) < self.max_length:\n            tokens += ['<pad>'] * (self.max_length - len(tokens))\n        else:\n            tokens = tokens[:self.max_length]\n        # Convert tokens to indices\n        indices = [self.glove_vocab[token] for token in tokens]\n        label = torch.tensor(self.labels[idx], dtype=torch.float)\n        return torch.tensor(indices, dtype=torch.long), label\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T22:34:00.436638Z","iopub.execute_input":"2024-12-08T22:34:00.437176Z","iopub.status.idle":"2024-12-08T22:34:00.445919Z","shell.execute_reply.started":"2024-12-08T22:34:00.437131Z","shell.execute_reply":"2024-12-08T22:34:00.444917Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def load_glove_vocab(filepath='../data/glove.6B/glove.6B.50d.txt'):\n    glove_vocab = {}\n    embeddings = []\n    with open(filepath, 'r', encoding='utf-8') as f:\n        for idx, line in enumerate(f):\n            values = line.split()\n            word = values[0]\n            vector = [float(x) for x in values[1:]]\n            glove_vocab[word] = idx\n            embeddings.append(vector)\n    glove_vocab['<pad>'] = len(embeddings)\n    embeddings.append([0.0] * len(vector))\n    glove_vocab['<unk>'] = len(embeddings)\n    embeddings.append([1.0] * len(vector))\n    return glove_vocab, torch.tensor(embeddings, dtype=torch.float)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T22:34:00.448219Z","iopub.execute_input":"2024-12-08T22:34:00.448583Z","iopub.status.idle":"2024-12-08T22:34:00.461322Z","shell.execute_reply.started":"2024-12-08T22:34:00.448545Z","shell.execute_reply":"2024-12-08T22:34:00.460652Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"glove_vocab, glove_embeddings = load_glove_vocab('/kaggle/input/gloveembed/glove.6B.100d.txt') #TODO: Tune for optimal distance (50, 100, 200, 300)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T22:34:00.462237Z","iopub.execute_input":"2024-12-08T22:34:00.462489Z","iopub.status.idle":"2024-12-08T22:34:15.642446Z","shell.execute_reply.started":"2024-12-08T22:34:00.462465Z","shell.execute_reply":"2024-12-08T22:34:15.641656Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\ndf = df.dropna(subset=['comment_text'])\ndf['target'] = df['target'].round(0).astype(int)\n# df = df[:int(0.5*len(df))]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T22:34:15.643548Z","iopub.execute_input":"2024-12-08T22:34:15.643877Z","iopub.status.idle":"2024-12-08T22:34:37.022320Z","shell.execute_reply.started":"2024-12-08T22:34:15.643842Z","shell.execute_reply":"2024-12-08T22:34:37.021660Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(df['comment_text'], df['target'], test_size=0.1, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T22:34:37.023411Z","iopub.execute_input":"2024-12-08T22:34:37.023748Z","iopub.status.idle":"2024-12-08T22:34:37.471113Z","shell.execute_reply.started":"2024-12-08T22:34:37.023706Z","shell.execute_reply":"2024-12-08T22:34:37.470382Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"train_dataset = JigsawDataset(X_train, y_train, glove_vocab, max_length=220)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T22:34:37.472209Z","iopub.execute_input":"2024-12-08T22:34:37.472578Z","iopub.status.idle":"2024-12-08T22:37:11.109412Z","shell.execute_reply.started":"2024-12-08T22:34:37.472540Z","shell.execute_reply":"2024-12-08T22:37:11.108663Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"test_dataset = JigsawDataset(X_test, y_test, glove_vocab, max_length=220)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T22:37:11.110300Z","iopub.execute_input":"2024-12-08T22:37:11.110620Z","iopub.status.idle":"2024-12-08T22:37:27.258214Z","shell.execute_reply.started":"2024-12-08T22:37:11.110593Z","shell.execute_reply":"2024-12-08T22:37:27.257518Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"for batch in test_loader:\n    inputs, labels = batch\n    print(\"Input shape:\", inputs)\n    print(\"Label shape:\", labels)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T22:37:27.261300Z","iopub.execute_input":"2024-12-08T22:37:27.261682Z","iopub.status.idle":"2024-12-08T22:37:27.359194Z","shell.execute_reply.started":"2024-12-08T22:37:27.261651Z","shell.execute_reply":"2024-12-08T22:37:27.358365Z"}},"outputs":[{"name":"stdout","text":"Input shape: tensor([[  3124,      1,   6380,  ..., 400000, 400000, 400000],\n        [  2747,      2,     41,  ..., 400000, 400000, 400000],\n        [  2970,     25,      0,  ..., 400000, 400000, 400000],\n        ...,\n        [  8198,    285,    188,  ..., 400000, 400000, 400000],\n        [  4832,  16201,    188,  ..., 400000, 400000, 400000],\n        [ 13408, 139150,     14,  ..., 400000, 400000, 400000]])\nLabel shape: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"### Model","metadata":{}},{"cell_type":"code","source":"class ToxicityClassifierLSTM(nn.Module):\n    def __init__(self, embedding_matrix, hidden_dim, output_dim, num_layers, dropout=None):\n        super(ToxicityClassifierLSTM, self).__init__()\n        vocab_size, embedding_dim = embedding_matrix.shape\n        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout if dropout is not None else 0)\n        self.lstm_layer_1 = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n        self.lstm_layer_2 = nn.LSTM(hidden_dim * 2, hidden_dim, bidirectional=True, batch_first=True)\n        self.linear1 = nn.Linear(4 * hidden_dim, 4 * hidden_dim)\n        self.linear2 = nn.Linear(4 * hidden_dim, 4 * hidden_dim)\n        self.fc = nn.Linear(4 * hidden_dim, output_dim)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        embedded = self.embedding(x)\n        \n        lstm_out, _ = self.lstm_layer_1(embedded)\n        lstm_out, _ = self.lstm_layer_2(lstm_out)\n        \n        # Global average pooling\n        avg_pool = torch.mean(lstm_out, 1)\n        # Global max pooling\n        max_pool, _ = torch.max(lstm_out, 1)\n        \n        pool_out = torch.cat((max_pool, avg_pool), 1)\n        linear1_out  = F.relu(self.linear1(pool_out))\n        linear2_out  = F.relu(self.linear2(pool_out))\n        \n        final_hidden_state = pool_out + linear1_out + linear2_out\n        \n        return self.fc(final_hidden_state)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T22:37:27.360281Z","iopub.execute_input":"2024-12-08T22:37:27.360603Z","iopub.status.idle":"2024-12-08T22:37:27.368473Z","shell.execute_reply.started":"2024-12-08T22:37:27.360576Z","shell.execute_reply":"2024-12-08T22:37:27.367574Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def train_step(model, train_loader, optimizer, criterion, device):\n    model.train()\n    epoch_loss = 0\n    all_preds = []\n    all_labels = []\n\n    for inputs, labels in tqdm(train_loader):\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n\n        predictions = model(inputs).squeeze(1)\n        loss = criterion(predictions, labels)\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()\n        all_preds.extend((predictions > 0.5).int().cpu().numpy())\n        all_labels.extend(labels.int().cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    return epoch_loss / len(train_loader), accuracy\n\n\ndef evaluate_step(model, test_loader, criterion, device):\n    model.eval()\n    epoch_loss = 0\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for inputs, labels in tqdm(test_loader):\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            predictions = model(inputs).squeeze(1)\n            loss = criterion(predictions, labels)\n\n            epoch_loss += loss.item()\n            all_preds.extend((predictions > 0.5).int().cpu().numpy())\n            all_labels.extend(labels.int().cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    return epoch_loss / len(test_loader), accuracy\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T22:37:27.369839Z","iopub.execute_input":"2024-12-08T22:37:27.370155Z","iopub.status.idle":"2024-12-08T22:37:27.380342Z","shell.execute_reply.started":"2024-12-08T22:37:27.370115Z","shell.execute_reply":"2024-12-08T22:37:27.379600Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def train(train_loader, test_loader, glove_vocab, glove_embeddings):\n    # Hyperparameters\n    HIDDEN_DIM = 128\n    OUTPUT_DIM = 1\n    NUM_LAYERS = 2\n    PAD_IDX = glove_vocab['<pad>']\n    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    # Initialize model\n    model = ToxicityClassifierLSTM(\n        embedding_matrix=glove_embeddings,\n        hidden_dim=HIDDEN_DIM,\n        output_dim=OUTPUT_DIM,\n        num_layers=NUM_LAYERS,\n    )\n    model.to(DEVICE)\n\n    # Copy pre-trained embeddings to the model's embedding layer\n    # pretrained_embeddings = torch.stack(list(glove_vocab.values()))\n    # model.embedding.weight.data.copy_(pretrained_embeddings)\n\n    # Optimizer and Loss\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    criterion = nn.BCEWithLogitsLoss(reduction='mean') #nn.BCELoss()\n\n    # Train and evaluate\n    EPOCHS = 3\n    for epoch in range(EPOCHS):\n        print(f\"Epoch {epoch + 1}\")\n        train_loss, train_acc = train_step(model, train_loader, optimizer, criterion, DEVICE)\n        print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n        test_loss, test_acc = evaluate_step(model, test_loader, criterion, DEVICE)\n        print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T22:37:27.381383Z","iopub.execute_input":"2024-12-08T22:37:27.381699Z","iopub.status.idle":"2024-12-08T22:37:27.394088Z","shell.execute_reply.started":"2024-12-08T22:37:27.381664Z","shell.execute_reply":"2024-12-08T22:37:27.393291Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"model = train(train_loader, test_loader, glove_vocab, glove_embeddings)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T22:37:27.395021Z","iopub.execute_input":"2024-12-08T22:37:27.395359Z","iopub.status.idle":"2024-12-08T23:26:16.211235Z","shell.execute_reply.started":"2024-12-08T22:37:27.395303Z","shell.execute_reply":"2024-12-08T23:26:16.210195Z"}},"outputs":[{"name":"stdout","text":"Epoch 1\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 50762/50762 [15:27<00:00, 54.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1000, Train Accuracy: 0.9620\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5641/5641 [00:47<00:00, 119.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.0919, Test Accuracy: 0.9621\nEpoch 2\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 50762/50762 [15:27<00:00, 54.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0882, Train Accuracy: 0.9654\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5641/5641 [00:46<00:00, 120.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.0875, Test Accuracy: 0.9656\nEpoch 3\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 50762/50762 [15:29<00:00, 54.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0841, Train Accuracy: 0.9667\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5641/5641 [00:46<00:00, 120.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.0928, Test Accuracy: 0.9663\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T23:35:33.443972Z","iopub.execute_input":"2024-12-08T23:35:33.444319Z","iopub.status.idle":"2024-12-08T23:35:33.448790Z","shell.execute_reply.started":"2024-12-08T23:35:33.444288Z","shell.execute_reply":"2024-12-08T23:35:33.447733Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n    \ndef predict(model, sentence, max_length=220):\n    tokens = test_dataset._preprocess(text=sentence)\n    if len(tokens) < max_length:\n        tokens += ['<pad>'] * (max_length - len(tokens))\n    else:\n        tokens = tokens[:max_length]\n    indices = torch.tensor([glove_vocab[token] for token in tokens], dtype=torch.long)\n    inputs = indices.to(DEVICE)\n    predictions = model(inputs.unsqueeze_(0)).squeeze(1)\n    \n    return sigmoid(predictions.detach().cpu().numpy())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T23:37:04.909430Z","iopub.execute_input":"2024-12-08T23:37:04.910417Z","iopub.status.idle":"2024-12-08T23:37:04.916406Z","shell.execute_reply.started":"2024-12-08T23:37:04.910361Z","shell.execute_reply":"2024-12-08T23:37:04.915508Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"sentence = \"\"\npredict(model, sentence)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T23:45:13.074627Z","iopub.execute_input":"2024-12-08T23:45:13.075625Z","iopub.status.idle":"2024-12-08T23:45:13.087806Z","shell.execute_reply.started":"2024-12-08T23:45:13.075577Z","shell.execute_reply":"2024-12-08T23:45:13.086874Z"}},"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"array([0.04150735], dtype=float32)"},"metadata":{}}],"execution_count":61},{"cell_type":"code","source":"torch.save(model.state_dict(), 'LSTM_1.pth') ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T23:34:30.740251Z","iopub.execute_input":"2024-12-08T23:34:30.740980Z","iopub.status.idle":"2024-12-08T23:34:31.084996Z","shell.execute_reply.started":"2024-12-08T23:34:30.740944Z","shell.execute_reply":"2024-12-08T23:34:31.084271Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}