{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this notebook is explore the dataset text in the lens of the embeddings. We want to maximize the words in the dataset that have embeddings and minimize out-of-vocabulary words. So essentially, this notebook contributes to finding the best preprocessing operations on the dataset that make it optimal given an embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import operator\n",
    "import pickle\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "from contractions import fix\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    This is so cool. It's like, 'would you want yo...\n",
       "1    Thank you!! This would make my life a lot less...\n",
       "2    This is such an urgent design problem; kudos t...\n",
       "3    Is this something I'll be able to install on m...\n",
       "4                 haha you guys are a bunch of losers.\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['comment_text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>...</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59848</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is such an urgent design problem; kudos t...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Is this something I'll be able to install on m...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59856</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.87234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id    target                                       comment_text  \\\n",
       "0  59848  0.000000  This is so cool. It's like, 'would you want yo...   \n",
       "1  59849  0.000000  Thank you!! This would make my life a lot less...   \n",
       "2  59852  0.000000  This is such an urgent design problem; kudos t...   \n",
       "3  59855  0.000000  Is this something I'll be able to install on m...   \n",
       "4  59856  0.893617               haha you guys are a bunch of losers.   \n",
       "\n",
       "   severe_toxicity  obscene  identity_attack   insult  threat  asian  atheist  \\\n",
       "0         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
       "1         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
       "2         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
       "3         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
       "4         0.021277      0.0         0.021277  0.87234     0.0    0.0      0.0   \n",
       "\n",
       "   ...  article_id    rating  funny  wow  sad  likes  disagree  \\\n",
       "0  ...        2006  rejected      0    0    0      0         0   \n",
       "1  ...        2006  rejected      0    0    0      0         0   \n",
       "2  ...        2006  rejected      0    0    0      0         0   \n",
       "3  ...        2006  rejected      0    0    0      0         0   \n",
       "4  ...        2006  rejected      0    0    0      1         0   \n",
       "\n",
       "   sexual_explicit  identity_annotator_count  toxicity_annotator_count  \n",
       "0              0.0                         0                         4  \n",
       "1              0.0                         0                         4  \n",
       "2              0.0                         0                         4  \n",
       "3              0.0                         0                         4  \n",
       "4              0.0                         4                        47  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping the rows where the comment is empty. Forntunately its only 3 of the ~180000 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(train_df['comment_text'].isnull().sum())\n",
    "train_df = train_df.dropna(subset=['comment_text'])\n",
    "print(train_df['comment_text'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_vocab(filepath='../data/glove.6B/glove.6B.50d.txt'):\n",
    "    glove_vocab = {}\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            glove_vocab[word] = np.array(values[1:], dtype='float32')\n",
    "    glove_vocab['<pad>'] = np.zeros(len(values)-1)  # Add padding token\n",
    "    return glove_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = load_glove_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary and Coverage Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences, verbose=True):\n",
    "    \"\"\"\n",
    "    :param sentences: list of list of words\n",
    "    :return: dictionary of words and their count\n",
    "    \"\"\"\n",
    "    vocab_count = {}\n",
    "    for sentence in tqdm(sentences, disable=(not verbose)):\n",
    "        if type(sentence) is str:\n",
    "            sentence = sentence.split()\n",
    "        for word in sentence:\n",
    "            if word in vocab_count:\n",
    "                vocab_count[word] += 1\n",
    "            else:\n",
    "                vocab_count[word] = 1\n",
    "    return vocab_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_coverage(vocab, embeddings):\n",
    "    oov = {}\n",
    "    embeddings_found_unique, embeddings_found_total, no_embeddings_unique, no_embeddings_total = 0, 0, 0, 0\n",
    "    for word in tqdm(vocab):\n",
    "        if word == '\"the':\n",
    "            print('Found')\n",
    "        if word in embeddings:\n",
    "            embeddings_found_unique += 1\n",
    "            embeddings_found_total += vocab[word]\n",
    "        else:\n",
    "            no_embeddings_unique += 1\n",
    "            no_embeddings_total += vocab[word]\n",
    "            oov[word] = vocab[word]\n",
    "\n",
    "\n",
    "    print('Found embeddings for {:.2%} of unique words'.format(embeddings_found_unique / len(vocab)))\n",
    "    print('Found embeddings for {:.2%} of all text'.format(embeddings_found_total / (embeddings_found_total + no_embeddings_total)))\n",
    "    \n",
    "    oov_in_order_of_decreasing_occurrence = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return oov_in_order_of_decreasing_occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1804871/1804871 [00:32<00:00, 55152.14it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_vocab = build_vocab(list(train_df['comment_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words:  1670966\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique words: ', len(dataset_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words occurring more than once:  596758\n"
     ]
    }
   ],
   "source": [
    "print('Number of words occurring more than once: ', len(dict((k, v) for k, v in dataset_vocab.items() if v > 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~70% of the words occur only once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1670966/1670966 [00:02<00:00, 777042.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 5.58% of unique words\n",
      "Found embeddings for 74.82% of all text\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(dataset_vocab, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly we only found embeddings for 5% of all the words in the text. These words (like 'the', 'a') occur often and hence the 75% count for all text. Let's see what words were not found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 861783),\n",
       " ('The', 435047),\n",
       " (\"don't\", 178881),\n",
       " ('Trump', 156956),\n",
       " ('It', 153815),\n",
       " ('You', 144381),\n",
       " ('If', 143987),\n",
       " ('And', 128132),\n",
       " ('This', 121363),\n",
       " (\"it's\", 100959)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see upper case letters are not found, and also the use of the apostrephe (') in contractions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Contractions and Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixContractionsAndConvertCase(sentence):\n",
    "    sentence = fix(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ur a sh*tty comment.  _______  you are a sh*tty comment.\n",
      "Is this something I'll be able to install on my site? When will you be releasing it?  _______  is this something i will be able to install on my site? when will you be releasing it?\n",
      "This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!  _______  this is so cool. it is like, 'would you want your mother to read this??' really great idea, well done!\n"
     ]
    }
   ],
   "source": [
    "sentence = train_df['comment_text'][5]\n",
    "print(sentence, ' _______ ', fixContractionsAndConvertCase(sentence))\n",
    "sentence = train_df['comment_text'][3]\n",
    "print(sentence, ' _______ ', fixContractionsAndConvertCase(sentence))\n",
    "sentence = train_df['comment_text'][0]\n",
    "print(sentence, ' _______ ', fixContractionsAndConvertCase(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty cool - contractions expanded and sentence converted to lower case. Let's check the coverage again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['processed_text'] = train_df['comment_text'].apply(lambda x: fixContractionsAndConvertCase(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1804871/1804871 [00:49<00:00, 36611.17it/s]\n",
      "100%|██████████| 1458321/1458321 [00:01<00:00, 861663.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 8.75% of unique words\n",
      "Found embeddings for 86.85% of all text\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('it.', 85295),\n",
       " ('them.', 37493),\n",
       " ('it,', 29869),\n",
       " ('that.', 28301),\n",
       " ('yes,', 27927),\n",
       " ('you.', 26861),\n",
       " ('not.', 26028),\n",
       " ('\"the', 25151),\n",
       " (\"trump's\", 24479),\n",
       " ('time.', 22364)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_vocab = build_vocab(list(train_df['processed_text']))\n",
    "oov = check_coverage(dataset_vocab, embeddings)\n",
    "oov[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have an improvement. In the out-of-vocabulary set we see cases of special characters not found in the embeddings. Let's handle these"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeSpecialCharacters(sentence):\n",
    "    sentence = re.sub(r'[_]', ' ', sentence)\n",
    "    # Removing some special characters that usually don't add meaning to a sentence\n",
    "    # sentence = re.sub(r\"[#*;[\\\\^`{|}~'\\\"]\", '', sentence)\n",
    "    sentence = re.sub(r\"[^?$.-:()%@!&=+/><,a-zA-Z\\s0-9\\w]\", '', sentence)\n",
    "    # Changes multiple occurrences of a character to one occurrence\n",
    "    sentence = re.sub(r'([?.!#$%&()*+,-/:;_<=>@[^`|])\\1+', r'\\1', sentence)\n",
    "    # Inserts a space before and after special characters\n",
    "    sentence = re.sub(r'([?.!#$%&()*+,-/:;_<=>@[^`|])', r' \\1 ', sentence)\n",
    "    # Removes extra spaces that may have come in from the previous operation\n",
    "    sentence = re.sub(r'([\\s])\\1+', r'\\1', sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing $50,999!! on_set numbers..\"the game is :/rigged\"  _______  Testing $ 50 , 999 ! on set numbers . the game is : / rigged\n",
      "is this something i will be able to install on my site ? when will you be releasing it ?   _______  is this something i will be able to install on my site ? when will you be releasing it ? \n",
      "this is so cool . it is like , would you want your mother to read this ? really great idea , well done !   _______  this is so cool . it is like , would you want your mother to read this ? really great idea , well done ! \n"
     ]
    }
   ],
   "source": [
    "sentence = 'Testing $50,999!! on_set numbers..\"the game is :/rigged\"'\n",
    "print(sentence, ' _______ ', removeSpecialCharacters(sentence))\n",
    "sentence = train_df['processed_text'][3]\n",
    "print(sentence, ' _______ ', removeSpecialCharacters(sentence))\n",
    "sentence = train_df['processed_text'][0]\n",
    "print(sentence, ' _______ ', removeSpecialCharacters(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['processed_text'] = train_df['processed_text'].apply(lambda x: removeSpecialCharacters(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1804871/1804871 [00:36<00:00, 48859.49it/s]\n",
      "100%|██████████| 326864/326864 [00:00<00:00, 969133.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 38.30% of unique words\n",
      "Found embeddings for 99.41% of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('trudeaus', 5060),\n",
       " ('alaskas', 4433),\n",
       " ('antifa', 2513),\n",
       " ('daca', 2509),\n",
       " ('brexit', 1888),\n",
       " ('hawaiis', 1880),\n",
       " ('siemian', 1870),\n",
       " ('sb21', 1852),\n",
       " ('theglobeandmail', 1354),\n",
       " ('washingtonpost', 1353)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_vocab = build_vocab(list(train_df['processed_text']))\n",
    "oov = check_coverage(dataset_vocab, embeddings)\n",
    "oov[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to be a good point. We have embeddings for over 99% of the training dataset. Most of the oov words are proper nouns or typos. I'm happy to mark these as unknown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK's tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "def tokenize(sentence):\n",
    "    sentence = fix(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    sentence = word_tokenize(sentence)\n",
    "    return ' '.join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['processed_text_2'] = train_df['comment_text'].apply(lambda x: tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeSpecialCharacters(sentence):\n",
    "    sentence = re.sub(r'[_]', ' ', sentence)\n",
    "    sentence = re.sub(r\"[^?.-:()%@!&=+/><,a-zA-Z\\s0-9\\w]\", '', sentence)\n",
    "    # Changes multiple occurrences of a character to one occurrence\n",
    "    sentence = re.sub(r'([?.!#$%&()*+,-/:;_<=>@[^`|])\\1+', r'\\1', sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['processed_text_2'] = train_df['processed_text_2'].apply(lambda x: removeSpecialCharacters(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1804871/1804871 [00:36<00:00, 49622.01it/s]\n",
      "100%|██████████| 508201/508201 [00:00<00:00, 854459.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 25.88% of unique words\n",
      "Found embeddings for 99.02% of all text\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('you.s', 23608),\n",
       " ('/www.youtube.com/watch', 4888),\n",
       " ('antifa', 2512),\n",
       " ('daca', 2414),\n",
       " ('altright', 2336),\n",
       " ('sb21', 2128),\n",
       " ('siemian', 1959),\n",
       " ('4.', 1857),\n",
       " ('brexit', 1768),\n",
       " ('ok.', 1718)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_vocab = build_vocab(list(train_df['processed_text_2']))\n",
    "oov = check_coverage(dataset_vocab, embeddings)\n",
    "oov[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          this is so cool . it is like , would you want ...\n",
       "1          thank you ! this would make my life a lot less...\n",
       "2          this is such an urgent design problem kudos to...\n",
       "3          is this something i will be able to install on...\n",
       "4                     haha you guys are a bunch of losers . \n",
       "                                 ...                        \n",
       "1804869    maybe the tax on things would be collected whe...\n",
       "1804870    what do you call people who still think the di...\n",
       "1804871    thank you , right or wrong , i am following yo...\n",
       "1804872    anyone who is quoted as having the following e...\n",
       "1804873    students defined as ebd are legally just as di...\n",
       "Name: processed_text, Length: 1804871, dtype: object"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['processed_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          this is so cool . it is like , would you want ...\n",
       "1          thank you ! ! this would make my life a lot le...\n",
       "2          this is such an urgent design problem  kudos t...\n",
       "3          is this something i will be able to install on...\n",
       "4                      haha you guys are a bunch of losers .\n",
       "                                 ...                        \n",
       "1804869    maybe the tax on  things  would be collected w...\n",
       "1804870    what do you call people who still think the di...\n",
       "1804871    thank you , , , right or wrong , , , i am foll...\n",
       "1804872    anyone who is quoted as having the following e...\n",
       "1804873    students defined as ebd are legally just as di...\n",
       "Name: processed_text_2, Length: 1804871, dtype: object"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['processed_text_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'len'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6108\\4181284090.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'processed_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mean'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'max'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'std'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\shrav\\anaconda3\\envs\\NLP\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6295\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6296\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6297\u001b[0m         ):\n\u001b[0;32m   6298\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6299\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'len'"
     ]
    }
   ],
   "source": [
    "train_df['processed_text'].str.split().len().agg(['mean', 'max', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_lens = []\n",
    "for sentence in train_df['processed_text'].tolist():\n",
    "    sentence_lens.append(len(sentence.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59.24561533760585, 356, 52.638089654525864)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_lens = np.array(sentence_lens)\n",
    "sentence_lens.mean(), sentence_lens.max(), sentence_lens.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
