{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this notebook is explore the dataset text in the lens of the embeddings. We want to maximize the words in the dataset that have embeddings and minimize out-of-vocabulary words. So essentially, this notebook contributes to finding the best preprocessing operations on the dataset that make it optimal given an embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import operator\n",
    "import pickle\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "from contractions import fix\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    This is so cool. It's like, 'would you want yo...\n",
       "1    Thank you!! This would make my life a lot less...\n",
       "2    This is such an urgent design problem; kudos t...\n",
       "3    Is this something I'll be able to install on m...\n",
       "4                 haha you guys are a bunch of losers.\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['comment_text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping the rows where the comment is empty. Forntunately its only 3 of the ~180000 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(train_df['comment_text'].isnull().sum())\n",
    "train_df = train_df.dropna(subset=['comment_text'])\n",
    "print(train_df['comment_text'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_vocab(filepath='../data/glove.6B/glove.6B.50d.txt'):\n",
    "    glove_vocab = {}\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            glove_vocab[word] = np.array(values[1:], dtype='float32')\n",
    "    glove_vocab['<pad>'] = np.zeros(len(values)-1)  # Add padding token\n",
    "    return glove_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = load_glove_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary and Coverage Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences, verbose=True):\n",
    "    \"\"\"\n",
    "    :param sentences: list of list of words\n",
    "    :return: dictionary of words and their count\n",
    "    \"\"\"\n",
    "    vocab_count = {}\n",
    "    for sentence in tqdm(sentences, disable=(not verbose)):\n",
    "        for word in sentence.split():\n",
    "            if word in vocab_count:\n",
    "                vocab_count[word] += 1\n",
    "            else:\n",
    "                vocab_count[word] = 1\n",
    "    return vocab_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_coverage(vocab, embeddings):\n",
    "    oov = {}\n",
    "    embeddings_found_unique, embeddings_found_total, no_embeddings_unique, no_embeddings_total = 0, 0, 0, 0\n",
    "    for word in tqdm(vocab):\n",
    "        if word == '\"the':\n",
    "            print('Found')\n",
    "        if word in embeddings:\n",
    "            embeddings_found_unique += 1\n",
    "            embeddings_found_total += vocab[word]\n",
    "        else:\n",
    "            no_embeddings_unique += 1\n",
    "            no_embeddings_total += vocab[word]\n",
    "            oov[word] = vocab[word]\n",
    "\n",
    "\n",
    "    print('Found embeddings for {:.2%} of unique words'.format(embeddings_found_unique / len(vocab)))\n",
    "    print('Found embeddings for {:.2%} of all text'.format(embeddings_found_total / (embeddings_found_total + no_embeddings_total)))\n",
    "    \n",
    "    oov_in_order_of_decreasing_occurrence = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return oov_in_order_of_decreasing_occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1804871/1804871 [00:37<00:00, 47886.90it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_vocab = build_vocab(list(train_df['comment_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1670966/1670966 [00:02<00:00, 777042.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 5.58% of unique words\n",
      "Found embeddings for 74.82% of all text\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(dataset_vocab, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly we only found embeddings for 5% of all the words in the text. These words (like 'the', 'a') occur often and hence the 75% count for all text. Let's see what words were not found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 861783),\n",
       " ('The', 435047),\n",
       " (\"don't\", 178881),\n",
       " ('Trump', 156956),\n",
       " ('It', 153815),\n",
       " ('You', 144381),\n",
       " ('If', 143987),\n",
       " ('And', 128132),\n",
       " ('This', 121363),\n",
       " (\"it's\", 100959)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see upper case letters are not found, and also the use of the apostrephe (') in contractions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Contractions and Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixContractionsAndConvertCase(sentence):\n",
    "    sentence = fix(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ur a sh*tty comment.  _______  you are a sh*tty comment.\n",
      "Is this something I'll be able to install on my site? When will you be releasing it?  _______  is this something i will be able to install on my site? when will you be releasing it?\n",
      "This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!  _______  this is so cool. it is like, 'would you want your mother to read this??' really great idea, well done!\n"
     ]
    }
   ],
   "source": [
    "sentence = train_df['comment_text'][5]\n",
    "print(sentence, ' _______ ', fixContractionsAndConvertCase(sentence))\n",
    "sentence = train_df['comment_text'][3]\n",
    "print(sentence, ' _______ ', fixContractionsAndConvertCase(sentence))\n",
    "sentence = train_df['comment_text'][0]\n",
    "print(sentence, ' _______ ', fixContractionsAndConvertCase(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty cool - contractions expanded and sentence converted to lower case. Let's check the coverage again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['processed_text'] = train_df['comment_text'].apply(lambda x: fixContractionsAndConvertCase(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1804871/1804871 [00:49<00:00, 36611.17it/s]\n",
      "100%|██████████| 1458321/1458321 [00:01<00:00, 861663.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 8.75% of unique words\n",
      "Found embeddings for 86.85% of all text\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('it.', 85295),\n",
       " ('them.', 37493),\n",
       " ('it,', 29869),\n",
       " ('that.', 28301),\n",
       " ('yes,', 27927),\n",
       " ('you.', 26861),\n",
       " ('not.', 26028),\n",
       " ('\"the', 25151),\n",
       " (\"trump's\", 24479),\n",
       " ('time.', 22364)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_vocab = build_vocab(list(train_df['processed_text']))\n",
    "oov = check_coverage(dataset_vocab, embeddings)\n",
    "oov[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have an improvement. In the out-of-vocabulary set we see cases of special characters not found in the embeddings. Let's handle these"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeSpecialCharacters(sentence):\n",
    "    sentence = re.sub(r'[_]', ' ', sentence)\n",
    "    # Removing some special characters that usually don't add meaning to a sentence\n",
    "    # sentence = re.sub(r\"[#*;[\\\\^`{|}~'\\\"]\", '', sentence)\n",
    "    sentence = re.sub(r\"[^?.-:()%@!&=+/><,a-zA-Z\\s0-9\\w]\", '', sentence)\n",
    "    # Changes multiple occurrences of a character to one occurrence\n",
    "    sentence = re.sub(r'([?.!#$%&()*+,-/:;_<=>@[^`|])\\1+', r'\\1', sentence)\n",
    "    # Inserts a space before and after special characters\n",
    "    sentence = re.sub(r'([?.!#$%&()*+,-/:;_<=>@[^`|])', r' \\1 ', sentence)\n",
    "    # Removes extra spaces that may have come in from the previous operation\n",
    "    sentence = re.sub(r'([\\s])\\1+', r'\\1', sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing #999!! on_set numbers..\"the game is :/rigged\"  _______  Testing 999 ! on set numbers . the game is : / rigged\n",
      "is this something i will be able to install on my site ? when will you be releasing it ?   _______  is this something i will be able to install on my site ? when will you be releasing it ? \n",
      "this is so cool . it is like , would you want your mother to read this ? really great idea , well done !   _______  this is so cool . it is like , would you want your mother to read this ? really great idea , well done ! \n"
     ]
    }
   ],
   "source": [
    "sentence = 'Testing #999!! on_set numbers..\"the game is :/rigged\"'\n",
    "print(sentence, ' _______ ', removeSpecialCharacters(sentence))\n",
    "sentence = train_df['processed_text'][3]\n",
    "print(sentence, ' _______ ', removeSpecialCharacters(sentence))\n",
    "sentence = train_df['processed_text'][0]\n",
    "print(sentence, ' _______ ', removeSpecialCharacters(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['processed_text'] = train_df['processed_text'].apply(lambda x: removeSpecialCharacters(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1804871/1804871 [00:36<00:00, 48859.49it/s]\n",
      "100%|██████████| 326864/326864 [00:00<00:00, 969133.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 38.30% of unique words\n",
      "Found embeddings for 99.41% of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('trudeaus', 5060),\n",
       " ('alaskas', 4433),\n",
       " ('antifa', 2513),\n",
       " ('daca', 2509),\n",
       " ('brexit', 1888),\n",
       " ('hawaiis', 1880),\n",
       " ('siemian', 1870),\n",
       " ('sb21', 1852),\n",
       " ('theglobeandmail', 1354),\n",
       " ('washingtonpost', 1353)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_vocab = build_vocab(list(train_df['processed_text']))\n",
    "oov = check_coverage(dataset_vocab, embeddings)\n",
    "oov[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to be a good point. We have embeddings for over 99% of the training dataset. Most of the oov words are proper nouns or typos. I'm happy to mark these as unknown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK's tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "def tokenize(sentence):\n",
    "    sentence = word_tokenize(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['processed_text_2'] = train_df['comment_text'].apply(lambda x: tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_vocab = build_vocab(list(train_df['processed_text_2']))\n",
    "oov = check_coverage(dataset_vocab, embeddings)\n",
    "oov[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
